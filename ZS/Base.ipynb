{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasos para armar modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Levantar la informaci贸n y definir el problema\n",
    "- Entrevista con stakeholders\n",
    "- Estudiar el contexto del problema\n",
    "- Definir m茅tricas claves del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Recolecci贸n de datos\n",
    "- Acceder a las bases de datos encesarias \n",
    "- Integrar datos de m煤ltiples fuentes si es necesario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import zscore\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librer铆as necesarias para an谩lisis de datos, visualizaci贸n y preprocesamiento\n",
    "import pandas as pd  # Manejo de datos tabulares\n",
    "import numpy as np  # Operaciones matem谩ticas y matrices\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder, RobustScaler  # Codificaci贸n y escalado\n",
    "from sklearn.impute import SimpleImputer, KNNImputer  # Imputaci贸n de valores nulos\n",
    "import seaborn as sns  # Visualizaci贸n de datos\n",
    "import matplotlib.pyplot as plt  # Gr谩ficos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Exploratory Data Analysis\n",
    "- An谩lisis descriptivo\n",
    "- Visualizaci贸n\n",
    "- etectar valores at铆picos y datos faltantes (nulos, outliers, duplicados, incnsistencias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Comprensi贸n inicial de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miro un poco la informaci贸n\n",
    "df = pd.read_csv(\"books.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtengo informaci贸n de cu谩ntos nulos hay y el tipo de dato\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estad铆stica descriptiva\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Me fijo el tipo de dato de cada uno, tengo que validar el tipo de dato tenga sentido. \n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifico la cantida de filas y columnas\n",
    "print(f\"Filas: {df.shape[0]}, Columnas: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Me fijo en las varaibles categ贸ricas qu茅 opciones hay y cu谩les hay de cada una\n",
    "print(df['variable_categorica'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 An谩lisis de valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Me fijo si hay nulos\n",
    "print(df.isna().sum())\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminaci贸n de filas/columnas: Si el porcentaje de nulos es alto (e.g., > 40%).\n",
    "\n",
    "Imputaci贸n:\n",
    "\n",
    "Num茅ricas: Media, mediana, moda o t茅cnicas avanzadas como KNN imputer (from sklearn.impute import KNNImputer).\n",
    "Categ贸ricas: Rellenar con la moda o una nueva categor铆a (\"Desconocido\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizo los valores nulos\n",
    "sns.heatmap(df.isnull(), cbar=False, cmap=\"viridis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 An谩lisis de outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reglas de 1.5*IQR\n",
    "Q1 = df['variable'].quantile(0.25)\n",
    "Q3 = df['variable'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers = df[(df['variable'] < Q1 - 1.5 * IQR) | (df['variable'] > Q3 + 1.5 * IQR)]\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-score\n",
    "from scipy.stats import zscore\n",
    "df['zscore'] = zscore(df['variable'])\n",
    "print(df[df['zscore'].abs() > 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tratamiento de outliers:\n",
    "\n",
    "- Eliminaci贸n: Si son errores claros o irrelevantes.\n",
    "- Winsorizaci贸n: Reemplazar valores extremos por percentiles (e.g., 5% y 95%).\n",
    "- Transformaciones: Aplicar logaritmo, ra铆z cuadrada o escalado robusto (RobustScaler de scikit-learn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci贸n de outliers\n",
    "sns.boxplot(x=df['variable'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.4 An谩lisis de correlaci贸n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de correlaci贸n\n",
    "corr_matrix = df.corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identificaci贸n de relaciones fuertes:\n",
    "\n",
    "Revisa valores de correlaci贸n altos (>0.8) o bajos (<-0.8) para eliminar variables redundantes (multicolinealidad).\n",
    "Si usas variables categ贸ricas, emplea Cramers V para analizar relaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot\n",
    "sns.pairplot(df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot\n",
    "sns.scatterplot(x='variable1', y='variable2', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.5 An谩lisis de variables categ贸ricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribuci贸n de categor铆as\n",
    "df['variable_categorica'].value_counts().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformaci贸n de variables categ贸ricas\n",
    "# Label Encoding (si hay odinalidad)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "df['variable_categorica'] = LabelEncoder().fit_transform(df['variable_categorica'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding (para categor铆as nominales)\n",
    "df = pd.get_dummies(df, columns=['variable_categorica'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.6 An谩lisis de distribuciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables num茅ricas\n",
    "# Histograma\n",
    "df['variable'].hist(bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para visualizar variables num茅ricas\n",
    "sns.histplot(data=df, x=\"varaible\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df, x=\"varaible\", binwidth=.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables num茅ricas\n",
    "# KDE (Kernel Density Estimation)\n",
    "sns.kdeplot(df['variable'], shade=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformaci贸n de datos\n",
    "# Escalado (para modelos sensibles como SVM o KNN)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "df['variable_scaled'] = StandardScaler().fit_transform(df[['variable']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.7 Interacci贸n entre varaibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An谩lisis bivariado\n",
    "# Categ贸rico vs. num茅rico\n",
    "sns.boxplot(x='categoria', y='variable_numerica', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dos categor铆as\n",
    "sns.scatterplot(x='variable1', y='variable2', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.8 Otras cosas a hacer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valores duplicados\n",
    "print(df.duplicated().sum())\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Preprocesamiento de Datos\n",
    "Objetivos:\n",
    "    Preparar los datos para que sean adecuados para el modelo.\n",
    "Acciones:\n",
    "1. Limpieza de datos:\n",
    "    Imputar valores nulos (por ejemplo, usar la media o modelos para estimarlos).\n",
    "    Corregir errores en los datos.\n",
    "    Eliminar duplicados.\n",
    "2. Codificaci贸n de variables categ贸ricas:\n",
    "    Usar t茅cnicas como One-Hot Encoding o codificaci贸n ordinal.\n",
    "3. Escalado y normalizaci贸n: Escalar datos num茅ricos (por ejemplo, MinMaxScaler o StandardScaler).\n",
    "4. Transformaci贸n: Corregir distribuciones sesgadas (aplicando logaritmos, ra铆ces cuadradas, etc.).\n",
    "5. Equilibrio de clases (si aplica): Usar t茅cnicas como SMOTE o undersampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1 Valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muestra la cantidad de valores nulos por columna\n",
    "print(\"Valores nulos por columna:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representaci贸n gr谩fica de los valores nulos como un mapa de calor\n",
    "sns.heatmap(df.isnull(), cbar=False, cmap=\"viridis\")\n",
    "plt.title(\"Mapa de calor de valores nulos\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reemplaza valores nulos de variables num茅ricas con la media (puede usarse mediana tambi茅n)\n",
    "num_imputer = SimpleImputer(strategy='mean')\n",
    "df['variable_numerica'] = num_imputer.fit_transform(df[['variable_numerica']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reemplaza valores nulos de variables categ贸ricas con el valor m谩s frecuente\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "df['variable_categorica'] = cat_imputer.fit_transform(df[['variable_categorica']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usa el algoritmo KNN para estimar y reemplazar valores nulos bas谩ndose en vecinos cercanos\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "df[['variable1', 'variable2']] = knn_imputer.fit_transform(df[['variable1', 'variable2']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3 Detecci贸n y tratamiento de outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula los l铆mites inferior y superior para detectar outliers usando la Regla de 1.5 * IQR\n",
    "Q1 = df['variable_numerica'].quantile(0.25)  # Primer cuartil\n",
    "Q3 = df['variable_numerica'].quantile(0.75)  # Tercer cuartil\n",
    "IQR = Q3 - Q1  # Rango intercuartil\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Identifica las filas que contienen valores fuera de los l铆mites\n",
    "outliers = df[(df['variable_numerica'] < lower_bound) | (df['variable_numerica'] > upper_bound)]\n",
    "print(f\"Cantidad de outliers: {len(outliers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gr谩fico tipo boxplot para visualizar los valores at铆picos\n",
    "sns.boxplot(x=df['variable_numerica'])\n",
    "plt.title(\"Detecci贸n de outliers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratamiento de outliers\n",
    "# Winsorizaci贸n\n",
    "# Limita los valores extremos al rango permitido (no elimina valores, solo los ajusta)\n",
    "df['variable_numerica'] = np.clip(df['variable_numerica'], lower_bound, upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratameinto de outliers\n",
    "# Elimino valores extemos\n",
    "# Filtra el dataframe para eliminar filas con valores at铆picos\n",
    "df = df[(df['variable_numerica'] >= lower_bound) & (df['variable_numerica'] <= upper_bound)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4 Codificaci贸n de variabls categ贸ricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labl Encoding (variables ordinales)\n",
    "# Convierte categor铆as en n煤meros enteros (煤til para variables categ贸ricas ordinales)\n",
    "label_encoder = LabelEncoder()\n",
    "df['variable_categorica'] = label_encoder.fit_transform(df['variable_categorica'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-Hot Encoding(varaibles nominales)\n",
    "# Crea nuevas columnas binarias para cada categor铆a (煤til para variables categ贸ricas nominales)\n",
    "df = pd.get_dummies(df, columns=['variable_categorica'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.5 Escalado de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalado est谩ndar (algritmos como SVM o PCA)\n",
    "# Normaliza los valores para que tengan media 0 y desviaci贸n est谩ndar 1\n",
    "scaler = StandardScaler()\n",
    "df['variable_numerica_escalada'] = scaler.fit_transform(df[['variable_numerica']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalado robusto (para datos con outliers)\n",
    "# Escala los valores considerando la mediana y el rango intercuartil (menos sensible a outliers)\n",
    "robust_scaler = RobustScaler()\n",
    "df['variable_numerica_robusta'] = robust_scaler.fit_transform(df[['variable_numerica']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.6 Generaci贸n de nuevas caracter铆sticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables polin贸micas\n",
    "# Crea una nueva columna con el valor al cuadrado y otra con el logaritmo natural\n",
    "df['variable_cuadrada'] = df['variable_numerica'] ** 2\n",
    "df['variable_log'] = np.log1p(df['variable_numerica'])  # Logaritmo con ajuste para evitar log(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Razones entre variables\n",
    "# Crea una nueva columna como la relaci贸n entre dos variables (evitando divisi贸n por 0)\n",
    "df['razon_variable'] = df['variable1'] / (df['variable2'] + 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning (agrupacion de valores continuos en categorias)\n",
    "# Agrupa valores continuos en categor铆as seg煤n rangos definidos\n",
    "df['rango_variable'] = pd.cut(df['variable_numerica'], bins=3, labels=['Bajo', 'Medio', 'Alto'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Divisi贸n de Datos\n",
    "Objetivos:\n",
    "    Separar los datos en conjuntos de entrenamiento, validaci贸n y prueba.\n",
    "Acciones:\n",
    "    Proporciones comunes: 70% entrenamiento, 15% validaci贸n, 15% prueba.\n",
    "Estrategias avanzadas:\n",
    "    Divisi贸n estratificada (para mantener proporciones de clases).\n",
    "    Validaci贸n cruzada (k-fold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las librer铆as necesarias para dividir los datos\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2. Cargar y preparar los datos\n",
    "Aseg煤rate de tener tus datos listos para ser divididos, con variables dependientes e independientes definidas. Aqu铆 tomamos como ejemplo que df es tu DataFrame y target es la columna objetivo (variable dependiente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir las variables independientes (X) y la variable dependiente (y)\n",
    "X = df.drop('target', axis=1)  # Eliminar la columna objetivo\n",
    "y = df['target']  # Solo la columna objetivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.3. Divisi贸n de datos en entrenamiento, validaci贸n y prueba\n",
    "a) Divisi贸n en entrenamiento y prueba\n",
    "Aqu铆 se divide en entrenamiento (70%) y prueba (30%), de modo que se tiene un conjunto de datos para entrenar el modelo y otro para evaluarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en un conjunto de entrenamiento (70%) y un conjunto de prueba (30%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "驴Por qu茅 70% entrenamiento y 30% prueba?\n",
    "Un 70% de datos para entrenamiento suele ser suficiente para entrenar el modelo sin sobreajustarlo.\n",
    "Un 30% para la prueba permite tener una muestra suficientemente grande para evaluar la precisi贸n del modelo.\n",
    "En algunos casos, se usa 80% para entrenamiento y 20% para prueba, pero este ajuste depende de cu谩ntos datos tengamos y de lo que busquemos en la evaluaci贸n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Divisi贸n de los datos de entrenamiento en entrenamiento y validaci贸n\n",
    "Posteriormente, el conjunto de entrenamiento (70%) se puede dividir en entrenamiento (85%) y validaci贸n (15%), de modo que podamos ajustar el modelo y validar su rendimiento durante el proceso de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir el conjunto de entrenamiento (70%) en subconjuntos de entrenamiento (85%) y validaci贸n (15%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "驴Por qu茅 15% para validaci贸n?\n",
    "Usar un 15% de los datos para validaci贸n es una opci贸n com煤n. Este conjunto de validaci贸n permite ajustar los par谩metros del modelo sin comprometer la capacidad de generalizaci贸n del modelo (sin usar los datos de prueba).\n",
    "Si tenemos pocos datos, podr铆amos optar por 10% para validaci贸n, pero en general un 15% proporciona un buen equilibrio para evaluar el rendimiento sin sobreajustar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.4. Resumen de las proporciones\n",
    "70% para entrenamiento: Permite que el modelo aprenda de una buena cantidad de datos.\n",
    "15% para validaci贸n: Permite ajustar hiperpar谩metros durante el entrenamiento.\n",
    "15% para prueba: Permite evaluar el rendimiento del modelo de forma objetiva y asegurarse de que no haya sobreajuste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar el tama帽o de cada conjunto de datos\n",
    "print(f\"Tama帽o de datos de entrenamiento: {X_train.shape[0]} filas\")\n",
    "print(f\"Tama帽o de datos de validaci贸n: {X_val.shape[0]} filas\")\n",
    "print(f\"Tama帽o de datos de prueba: {X_test.shape[0]} filas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.5. Alternativa con validaci贸n cruzada (K-fold cross-validation)\n",
    "Si prefieres no tener un conjunto de validaci贸n fijo, puedes usar validaci贸n cruzada (K-fold cross-validation) para evaluar el modelo durante el entrenamiento. Esta es una buena opci贸n si tienes un conjunto de datos peque帽o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Realizar validaci贸n cruzada con 5 pliegues\n",
    "scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "print(f\"Scores de validaci贸n cruzada: {scores}\")\n",
    "print(f\"Precisi贸n promedio: {scores.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "驴Por qu茅 usar validaci贸n cruzada?\n",
    "La validaci贸n cruzada utiliza diferentes subconjuntos del conjunto de entrenamiento para entrenar y validar el modelo varias veces. Esto da una estimaci贸n m谩s robusta de c贸mo el modelo podr铆a comportarse en datos nuevos, especialmente si tenemos pocos datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.6. Conclusi贸n\n",
    "La divisi贸n de datos en entrenamiento, validaci贸n y prueba es crucial para evitar el sobreajuste y garantizar que el modelo generalice correctamente a nuevos datos. Los pasos comunes son:\n",
    "\n",
    "Dividir en entrenamiento (70%) y prueba (30%).\n",
    "Dividir el conjunto de entrenamiento (70%) en entrenamiento (85%) y validaci贸n (15%).\n",
    "Usar validaci贸n cruzada si tienes pocos datos.\n",
    "Este proceso ayuda a tener datos suficientes para entrenar, ajustar y evaluar el modelo de forma adecuada, manteniendo una buena capacidad de generalizaci贸n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Selecci贸n del Modelo\n",
    "Objetivos:\n",
    "    Elegir un modelo adecuado para el problema y los datos.\n",
    "Acciones:\n",
    "    Determinar el tipo de modelo:\n",
    "        Regresi贸n (si la salida es num茅rica).\n",
    "        Clasificaci贸n (si la salida es categ贸rica).\n",
    "        Clustering o reducci贸n de dimensionalidad (si es no supervisado).\n",
    "Explorar m煤ltiples modelos: Comparar algoritmos como:\n",
    "    Clasificaci贸n: Random Forest, SVM, XGBoost.\n",
    "    Regresi贸n: Regresi贸n Lineal, Gradient Boosting.\n",
    "    Clustering: k-Means, DBSCAN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.1. Regresi贸n Lineal (Supervisado - Regresi贸n)\n",
    "Este modelo se usa cuando la variable dependiente es continua."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Datos de ejemplo\n",
    "X = df.drop('target', axis=1)  # Variables independientes\n",
    "y = df['target']  # Variable dependiente\n",
    "\n",
    "# Divisi贸n en entrenamiento y prueba (70% entrenamiento, 30% prueba)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Modelo de regresi贸n lineal\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluaci贸n del modelo\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Error cuadr谩tico medio (MSE): {mse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicaci贸n:\n",
    "\n",
    "El modelo LinearRegression() ajusta una l铆nea recta a los datos de entrenamiento.\n",
    "Usamos el MSE para evaluar la precisi贸n del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.2. Regresi贸n con Random Forest (Supervisado - Regresi贸n)\n",
    "Este modelo es 煤til cuando las relaciones no son lineales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Modelo de Random Forest para regresi贸n\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluaci贸n del modelo\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "print(f\"Error cuadr谩tico medio (MSE) - Random Forest: {mse_rf}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicaci贸n:\n",
    "\n",
    "RandomForestRegressor() crea un conjunto de 谩rboles de decisi贸n y promedia sus predicciones para mejorar la precisi贸n y evitar el sobreajuste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relaciones no son lineales en el contexto de Random Forest (y en general, en machine learning), se refiere a que la relaci贸n entre las caracter铆sticas (variables independientes) y la variable objetivo (dependiente) no sigue una estructura matem谩tica sencilla, como una l铆nea recta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.3. Clasificaci贸n con Regresi贸n Log铆stica (Supervisado - Clasificaci贸n)\n",
    "Este modelo es 煤til para problemas de clasificaci贸n binaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Datos de ejemplo\n",
    "X = df.drop('target', axis=1)  # Variables independientes\n",
    "y = df['target']  # Variable dependiente (binaria)\n",
    "\n",
    "# Divisi贸n en entrenamiento y prueba (70% entrenamiento, 30% prueba)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Modelo de regresi贸n log铆stica\n",
    "logreg_model = LogisticRegression()\n",
    "logreg_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred_logreg = logreg_model.predict(X_test)\n",
    "\n",
    "# Evaluaci贸n del modelo\n",
    "accuracy = accuracy_score(y_test, y_pred_logreg)\n",
    "print(f\"Precisi贸n de la Regresi贸n Log铆stica: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicaci贸n:\n",
    "\n",
    "LogisticRegression() es un modelo lineal usado para predecir probabilidades, ideal para clasificaci贸n binaria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.5. XGBoost (Supervisado - Regresi贸n y Clasificaci贸n)\n",
    "XGBoost es un potente algoritmo basado en el boosting, usado para problemas tanto de regresi贸n como de clasificaci贸n.\n",
    "\n",
    "C贸digo (para Regresi贸n):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Modelo de XGBoost para regresi贸n\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluaci贸n del modelo\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "print(f\"Error cuadr谩tico medio (MSE) - XGBoost: {mse_xgb}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C贸digo (para Clasificaci贸n):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Modelo de XGBoost para clasificaci贸n\n",
    "xgb_clf_model = xgb.XGBClassifier(objective='binary:logistic', n_estimators=100)\n",
    "xgb_clf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred_xgb_clf = xgb_clf_model.predict(X_test)\n",
    "\n",
    "# Evaluaci贸n del modelo\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb_clf)\n",
    "print(f\"Precisi贸n de XGBoost (Clasificaci贸n): {accuracy_xgb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicaci贸n:\n",
    "\n",
    "XGBRegressor() y XGBClassifier() son las implementaciones de XGBoost para problemas de regresi贸n y clasificaci贸n respectivamente. XGBoost es muy eficiente para manejar grandes cantidades de datos y evitar el sobreajuste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo del boosting es combinar m煤ltiples modelos (usualmente modelos simples, como 谩rboles de decisi贸n) para formar un modelo final m谩s preciso y robusto.\n",
    "Cada modelo sucesivo corrige los errores cometidos por el modelo anterior. En otras palabras, cada nuevo modelo se enfoca en mejorar las predicciones en los ejemplos que los modelos anteriores han clasificado incorrectamente.\n",
    "\n",
    "Tipos de modeos:\n",
    "- AdaBoost\n",
    "- Gradient Boosting\n",
    "- XGBoost\n",
    "- LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.6. Support Vector Machine (SVM - Supervisado - Clasificaci贸n y Regresi贸n)\n",
    "El SVM puede ser utilizado tanto para clasificaci贸n como para regresi贸n.\n",
    "\n",
    "C贸digo para Clasificaci贸n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Modelo SVM para clasificaci贸n\n",
    "svm_model = SVC(kernel='linear', random_state=42)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluaci贸n del modelo\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(f\"Precisi贸n del SVM: {accuracy_svm}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C贸digo para Regresi贸n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Modelo SVM para regresi贸n\n",
    "svr_model = SVR(kernel='linear')\n",
    "svr_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred_svr = svr_model.predict(X_test)\n",
    "\n",
    "# Evaluaci贸n del modelo\n",
    "mse_svr = mean_squared_error(y_test, y_pred_svr)\n",
    "print(f\"Error cuadr谩tico medio (MSE) - SVM: {mse_svr}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicaci贸n:\n",
    "\n",
    "SVC() y SVR() son las versiones de SVM para clasificaci贸n y regresi贸n respectivamente. SVM es eficaz en problemas donde las clases no son linealmente separables y cuando el n煤mero de caracter铆sticas es alto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clases no son linealmente separables, se hace referencia a un escenario en el que las dos clases a predecir no se pueden separar por una l铆nea recta (o un hiperplano en dimensiones superiores) en el espacio de caracter铆sticas. Esto significa que no es posible encontrar un 煤nico hiperplano que divida de manera perfecta las dos clases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagina un conjunto de datos donde los puntos de una clase est谩n agrupados alrededor de un c铆rculo y los de la otra clase est谩n agrupados en la zona exterior del c铆rculo. Una l铆nea recta no puede separar las clases de manera efectiva en este caso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kernel lineal:\n",
    "Descripci贸n: Este es el kernel m谩s simple y se utiliza cuando las clases son linealmente separables, o cuando quieres que el modelo funcione en el espacio original de caracter铆sticas sin aplicar ninguna transformaci贸n.\n",
    "Uso: Se usa cuando el conjunto de datos tiene una separaci贸n clara y lineal entre las clases.\n",
    "\n",
    "- Kernel polin贸mico:\n",
    "Descripci贸n: Este kernel mapea los datos a un espacio de mayor dimensi贸n de forma polin贸mica, donde \n",
    "\n",
    "d es el grado del polinomio, y \n",
    "\n",
    "c es un par谩metro de sesgo.\n",
    "Uso: Es 煤til cuando las relaciones entre las clases pueden representarse como una combinaci贸n polin贸mica de las caracter铆sticas.\n",
    "\n",
    "- Kernel Radial Basis Function (RBF) o Gaussiano\n",
    "Descripci贸n: El kernel RBF mide la similitud entre dos puntos, tomando en cuenta la distancia euclidiana entre ellos. Este kernel transforma los datos de manera no lineal, lo que permite que el SVM cree una frontera de decisi贸n muy flexible, adecuada para casos de clases no linealmente separables.\n",
    "Uso: Es el kernel m谩s com煤n en situaciones donde las clases no son linealmente separables, ya que puede mapear los datos a un espacio de caracter铆sticas de alta dimensi贸n y crear fronteras de decisi贸n complejas y no lineales.\n",
    "\n",
    "- Kernel Sigmoide\n",
    "Descripci贸n: Este kernel se basa en la funci贸n sigmoide y se comporta de manera similar a una red neuronal de una sola capa. Sin embargo, a menudo no se utiliza tanto como el RBF.\n",
    "Uso: Es 煤til en redes neuronales y en situaciones donde se busca modelar la no linealidad en las relaciones de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si tienes motivos para creer que las relaciones entre las caracter铆sticas y la clase no pueden separarse por una l铆nea recta (como en el caso de datos de imagen, texto o datos con interacciones complejas), el kernel RBF es una opci贸n com煤n debido a su flexibilidad y su capacidad para manejar datos con patrones complejos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.7. Reducci贸n de Dimensionalidad con PCA (No Supervisado)\n",
    "La reducci贸n de dimensionalidad es 煤til para manejar datos con muchas variables, manteniendo la mayor cantidad de informaci贸n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Datos de ejemplo\n",
    "X = df.drop('target', axis=1)\n",
    "\n",
    "# PCA: Reducci贸n a 2 dimensiones\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Visualizaci贸n de los datos proyectados\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1])\n",
    "plt.title(\"Reducci贸n de Dimensionalidad con PCA\")\n",
    "plt.xlabel(\"Componente Principal 1\")\n",
    "plt.ylabel(\"Componente Principal 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicaci贸n:\n",
    "\n",
    "PCA() reduce el n煤mero de caracter铆sticas, transformando los datos a un espacio de menor dimensi贸n. Es 煤til cuando se tienen muchas caracter铆sticas y se desea simplificar el modelo sin perder demasiada informaci贸n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducci贸n de simensionaldiad:\n",
    "- Se reduce el numeor de variables\n",
    "- cuando hay muchas caracter铆sticas uqe pued ellevar l sobreajuste\n",
    "- Elimina caracter铆sticas irrelevantes\n",
    "- Mejora eficiencia computacional\n",
    "- Previene el sobreajuste\n",
    "- Mejora la visualzaici贸n\n",
    "- Disminuye la dimensionalidad\n",
    "- Tecnicas son\n",
    "    - PCA\n",
    "    - t-SNE\n",
    "    - Autoencoders\n",
    "- Se usa cuando:\n",
    "    - los datos tienen muchas caracteristicas y se desea mejorar la intepretabilidad\n",
    "    cuando las caracteristicas tienen mucha correlacion\n",
    "    cuando se necesita una mejora en la eficiencia computacional\n",
    "    cuando le modelo muestra se帽ales de sobreajuste. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.8. Clustering con K-Means (No Supervisado)\n",
    "El clustering es 煤til cuando queremos agrupar datos en categor铆as sin tener etiquetas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Datos de ejemplo\n",
    "X = df.drop('target', axis=1)\n",
    "\n",
    "# K-Means\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Visualizaci贸n de los clusters\n",
    "plt.scatter(X['feature1'], X['feature2'], c=kmeans.labels_, cmap='viridis')\n",
    "plt.title(\"Clusters con K-Means\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicaci贸n:\n",
    "\n",
    "KMeans() agrupa los datos en el n煤mero de clusters especificado. Los puntos dentro de cada cluster son m谩s similares entre s铆 que a los puntos de otros clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.9 KNN (clasificaci贸n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaci贸n de bibliotecas necesarias\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Cargar datos de ejemplo (Iris dataset)\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Preprocesamiento: Est谩ndarizar las caracter铆sticas\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Dividir el conjunto de datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Crear el modelo KNN con k=3\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Entrenar el modelo\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Evaluar el modelo\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Entrenamiento del Modelo\n",
    "Objetivos:\n",
    "    Ajustar los par谩metros del modelo a los datos de entrenamiento.\n",
    "Acciones:\n",
    "    Entrenar con el conjunto de entrenamiento.\n",
    "    Ajustar par谩metros iniciales.\n",
    "    Guardar modelos iniciales para evaluaci贸n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Evaluaci贸n Inicial\n",
    "Objetivos:\n",
    "Medir el rendimiento del modelo con datos de validaci贸n.\n",
    "Acciones:\n",
    "M茅tricas comunes:\n",
    "Clasificaci贸n: Accuracy, Precision, Recall, F1, ROC-AUC.\n",
    "Regresi贸n: RMSE, MAE, R虏.\n",
    "Comparar varios modelos: Identificar cu谩l tiene el mejor desempe帽o."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Optimizaci贸n del Modelo\n",
    "Objetivos:\n",
    "Mejorar el rendimiento del modelo.\n",
    "Acciones:\n",
    "Ajuste de hiperpar谩metros:\n",
    "Usar t茅cnicas como GridSearchCV o RandomizedSearchCV.\n",
    "Reducci贸n de sobreajuste:\n",
    "Regularizaci贸n (L1, L2).\n",
    "Simplificar el modelo o reducir el n煤mero de caracter铆sticas.\n",
    "Ensamblado: Combinar modelos para mejorar la precisi贸n (bagging, boosting, stacking)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Evaluaci贸n Final con Datos de Prueba\n",
    "Objetivos:\n",
    "Verificar el desempe帽o en datos no vistos.\n",
    "Acciones:\n",
    "Usar el conjunto de prueba: Obtener m茅tricas finales de rendimiento.\n",
    "Comparar con la l铆nea base (baseline): Evaluar si el modelo supera m茅todos simples como un promedio o un modelo aleatorio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Implementaci贸n\n",
    "Objetivos:\n",
    "Desplegar el modelo en un entorno real para uso pr谩ctico.\n",
    "Acciones:\n",
    "Crear una API: Usar Flask, FastAPI o Django para servir el modelo.\n",
    "Automatizar predicciones: Integrar con sistemas existentes.\n",
    "Alojar el modelo: Usar servicios en la nube (AWS, Azure, GCP) o on-premises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Monitoreo y Mantenimiento\n",
    "Objetivos:\n",
    "Asegurar que el modelo sigue siendo 煤til y preciso.\n",
    "Acciones:\n",
    "Monitoreo de desempe帽o: Revisar m茅tricas en tiempo real.\n",
    "Identificar data drift: Detectar si los datos de entrada han cambiado.\n",
    "Reentrenar el modelo: Actualizarlo peri贸dicamente con nuevos datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ac谩 termina"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
