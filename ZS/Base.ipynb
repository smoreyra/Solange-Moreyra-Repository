{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasos para armar modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Levantar la información y definir el problema\n",
    "- Entrevista con stakeholders\n",
    "- Estudiar el contexto del problema\n",
    "- Definir métricas claves del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Recolección de datos\n",
    "- Acceder a las bases de datos encesarias \n",
    "- Integrar datos de múltiples fuentes si es necesario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import zscore\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías necesarias para análisis de datos, visualización y preprocesamiento\n",
    "import pandas as pd  # Manejo de datos tabulares\n",
    "import numpy as np  # Operaciones matemáticas y matrices\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder, RobustScaler  # Codificación y escalado\n",
    "from sklearn.impute import SimpleImputer, KNNImputer  # Imputación de valores nulos\n",
    "import seaborn as sns  # Visualización de datos\n",
    "import matplotlib.pyplot as plt  # Gráficos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Exploratory Data Analysis\n",
    "- Análisis descriptivo\n",
    "- Visualización\n",
    "- etectar valores atípicos y datos faltantes (nulos, outliers, duplicados, incnsistencias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Comprensión inicial de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miro un poco la información\n",
    "df = pd.read_csv(\"books.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtengo información de cuántos nulos hay y el tipo de dato\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estadística descriptiva\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Me fijo el tipo de dato de cada uno, tengo que validar el tipo de dato tenga sentido. \n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifico la cantida de filas y columnas\n",
    "print(f\"Filas: {df.shape[0]}, Columnas: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Me fijo en las varaibles categóricas qué opciones hay y cuáles hay de cada una\n",
    "print(df['variable_categorica'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Análisis de valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Me fijo si hay nulos\n",
    "print(df.isna().sum())\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminación de filas/columnas: Si el porcentaje de nulos es alto (e.g., > 40%).\n",
    "\n",
    "Imputación:\n",
    "\n",
    "Numéricas: Media, mediana, moda o técnicas avanzadas como KNN imputer (from sklearn.impute import KNNImputer).\n",
    "Categóricas: Rellenar con la moda o una nueva categoría (\"Desconocido\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizo los valores nulos\n",
    "sns.heatmap(df.isnull(), cbar=False, cmap=\"viridis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 Análisis de outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reglas de 1.5*IQR\n",
    "Q1 = df['variable'].quantile(0.25)\n",
    "Q3 = df['variable'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers = df[(df['variable'] < Q1 - 1.5 * IQR) | (df['variable'] > Q3 + 1.5 * IQR)]\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-score\n",
    "from scipy.stats import zscore\n",
    "df['zscore'] = zscore(df['variable'])\n",
    "print(df[df['zscore'].abs() > 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tratamiento de outliers:\n",
    "\n",
    "- Eliminación: Si son errores claros o irrelevantes.\n",
    "- Winsorización: Reemplazar valores extremos por percentiles (e.g., 5% y 95%).\n",
    "- Transformaciones: Aplicar logaritmo, raíz cuadrada o escalado robusto (RobustScaler de scikit-learn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de outliers\n",
    "sns.boxplot(x=df['variable'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.4 Análisis de correlación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de correlación\n",
    "corr_matrix = df.corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identificación de relaciones fuertes:\n",
    "\n",
    "Revisa valores de correlación altos (>0.8) o bajos (<-0.8) para eliminar variables redundantes (multicolinealidad).\n",
    "Si usas variables categóricas, emplea Cramer’s V para analizar relaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot\n",
    "sns.pairplot(df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot\n",
    "sns.scatterplot(x='variable1', y='variable2', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.5 Análisis de variables categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribución de categorías\n",
    "df['variable_categorica'].value_counts().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformación de variables categóricas\n",
    "# Label Encoding (si hay odinalidad)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "df['variable_categorica'] = LabelEncoder().fit_transform(df['variable_categorica'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding (para categorías nominales)\n",
    "df = pd.get_dummies(df, columns=['variable_categorica'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.6 Análisis de distribuciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables numéricas\n",
    "# Histograma\n",
    "df['variable'].hist(bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para visualizar variables numéricas\n",
    "sns.histplot(data=df, x=\"varaible\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df, x=\"varaible\", binwidth=.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables numéricas\n",
    "# KDE (Kernel Density Estimation)\n",
    "sns.kdeplot(df['variable'], shade=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformación de datos\n",
    "# Escalado (para modelos sensibles como SVM o KNN)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "df['variable_scaled'] = StandardScaler().fit_transform(df[['variable']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.7 Interacción entre varaibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis bivariado\n",
    "# Categórico vs. numérico\n",
    "sns.boxplot(x='categoria', y='variable_numerica', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dos categorías\n",
    "sns.scatterplot(x='variable1', y='variable2', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.8 Otras cosas a hacer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valores duplicados\n",
    "print(df.duplicated().sum())\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Preprocesamiento de Datos\n",
    "Objetivos:\n",
    "    Preparar los datos para que sean adecuados para el modelo.\n",
    "Acciones:\n",
    "1. Limpieza de datos:\n",
    "    Imputar valores nulos (por ejemplo, usar la media o modelos para estimarlos).\n",
    "    Corregir errores en los datos.\n",
    "    Eliminar duplicados.\n",
    "2. Codificación de variables categóricas:\n",
    "    Usar técnicas como One-Hot Encoding o codificación ordinal.\n",
    "3. Escalado y normalización: Escalar datos numéricos (por ejemplo, MinMaxScaler o StandardScaler).\n",
    "4. Transformación: Corregir distribuciones sesgadas (aplicando logaritmos, raíces cuadradas, etc.).\n",
    "5. Equilibrio de clases (si aplica): Usar técnicas como SMOTE o undersampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1 Valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muestra la cantidad de valores nulos por columna\n",
    "print(\"Valores nulos por columna:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representación gráfica de los valores nulos como un mapa de calor\n",
    "sns.heatmap(df.isnull(), cbar=False, cmap=\"viridis\")\n",
    "plt.title(\"Mapa de calor de valores nulos\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reemplaza valores nulos de variables numéricas con la media (puede usarse mediana también)\n",
    "num_imputer = SimpleImputer(strategy='mean')\n",
    "df['variable_numerica'] = num_imputer.fit_transform(df[['variable_numerica']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reemplaza valores nulos de variables categóricas con el valor más frecuente\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "df['variable_categorica'] = cat_imputer.fit_transform(df[['variable_categorica']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usa el algoritmo KNN para estimar y reemplazar valores nulos basándose en vecinos cercanos\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "df[['variable1', 'variable2']] = knn_imputer.fit_transform(df[['variable1', 'variable2']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3 Detección y tratamiento de outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula los límites inferior y superior para detectar outliers usando la Regla de 1.5 * IQR\n",
    "Q1 = df['variable_numerica'].quantile(0.25)  # Primer cuartil\n",
    "Q3 = df['variable_numerica'].quantile(0.75)  # Tercer cuartil\n",
    "IQR = Q3 - Q1  # Rango intercuartil\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Identifica las filas que contienen valores fuera de los límites\n",
    "outliers = df[(df['variable_numerica'] < lower_bound) | (df['variable_numerica'] > upper_bound)]\n",
    "print(f\"Cantidad de outliers: {len(outliers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico tipo boxplot para visualizar los valores atípicos\n",
    "sns.boxplot(x=df['variable_numerica'])\n",
    "plt.title(\"Detección de outliers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratamiento de outliers\n",
    "# Winsorización\n",
    "# Limita los valores extremos al rango permitido (no elimina valores, solo los ajusta)\n",
    "df['variable_numerica'] = np.clip(df['variable_numerica'], lower_bound, upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratameinto de outliers\n",
    "# Elimino valores extemos\n",
    "# Filtra el dataframe para eliminar filas con valores atípicos\n",
    "df = df[(df['variable_numerica'] >= lower_bound) & (df['variable_numerica'] <= upper_bound)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4 Codificación de variabls categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labl Encoding (variables ordinales)\n",
    "# Convierte categorías en números enteros (útil para variables categóricas ordinales)\n",
    "label_encoder = LabelEncoder()\n",
    "df['variable_categorica'] = label_encoder.fit_transform(df['variable_categorica'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-Hot Encoding(varaibles nominales)\n",
    "# Crea nuevas columnas binarias para cada categoría (útil para variables categóricas nominales)\n",
    "df = pd.get_dummies(df, columns=['variable_categorica'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.5 Escalado de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalado estándar (algritmos como SVM o PCA)\n",
    "# Normaliza los valores para que tengan media 0 y desviación estándar 1\n",
    "scaler = StandardScaler()\n",
    "df['variable_numerica_escalada'] = scaler.fit_transform(df[['variable_numerica']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalado robusto (para datos con outliers)\n",
    "# Escala los valores considerando la mediana y el rango intercuartil (menos sensible a outliers)\n",
    "robust_scaler = RobustScaler()\n",
    "df['variable_numerica_robusta'] = robust_scaler.fit_transform(df[['variable_numerica']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.6 Generación de nuevas características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables polinómicas\n",
    "# Crea una nueva columna con el valor al cuadrado y otra con el logaritmo natural\n",
    "df['variable_cuadrada'] = df['variable_numerica'] ** 2\n",
    "df['variable_log'] = np.log1p(df['variable_numerica'])  # Logaritmo con ajuste para evitar log(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Razones entre variables\n",
    "# Crea una nueva columna como la relación entre dos variables (evitando división por 0)\n",
    "df['razon_variable'] = df['variable1'] / (df['variable2'] + 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning (agrupacion de valores continuos en categorias)\n",
    "# Agrupa valores continuos en categorías según rangos definidos\n",
    "df['rango_variable'] = pd.cut(df['variable_numerica'], bins=3, labels=['Bajo', 'Medio', 'Alto'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. División de Datos\n",
    "Objetivos:\n",
    "    Separar los datos en conjuntos de entrenamiento, validación y prueba.\n",
    "Acciones:\n",
    "    Proporciones comunes: 70% entrenamiento, 15% validación, 15% prueba.\n",
    "Estrategias avanzadas:\n",
    "    División estratificada (para mantener proporciones de clases).\n",
    "    Validación cruzada (k-fold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las librerías necesarias para dividir los datos\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2. Cargar y preparar los datos\n",
    "Asegúrate de tener tus datos listos para ser divididos, con variables dependientes e independientes definidas. Aquí tomamos como ejemplo que df es tu DataFrame y target es la columna objetivo (variable dependiente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir las variables independientes (X) y la variable dependiente (y)\n",
    "X = df.drop('target', axis=1)  # Eliminar la columna objetivo\n",
    "y = df['target']  # Solo la columna objetivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.3. División de datos en entrenamiento, validación y prueba\n",
    "a) División en entrenamiento y prueba\n",
    "Aquí se divide en entrenamiento (70%) y prueba (30%), de modo que se tiene un conjunto de datos para entrenar el modelo y otro para evaluarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en un conjunto de entrenamiento (70%) y un conjunto de prueba (30%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Por qué 70% entrenamiento y 30% prueba?\n",
    "Un 70% de datos para entrenamiento suele ser suficiente para entrenar el modelo sin sobreajustarlo.\n",
    "Un 30% para la prueba permite tener una muestra suficientemente grande para evaluar la precisión del modelo.\n",
    "En algunos casos, se usa 80% para entrenamiento y 20% para prueba, pero este ajuste depende de cuántos datos tengamos y de lo que busquemos en la evaluación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) División de los datos de entrenamiento en entrenamiento y validación\n",
    "Posteriormente, el conjunto de entrenamiento (70%) se puede dividir en entrenamiento (85%) y validación (15%), de modo que podamos ajustar el modelo y validar su rendimiento durante el proceso de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir el conjunto de entrenamiento (70%) en subconjuntos de entrenamiento (85%) y validación (15%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Por qué 15% para validación?\n",
    "Usar un 15% de los datos para validación es una opción común. Este conjunto de validación permite ajustar los parámetros del modelo sin comprometer la capacidad de generalización del modelo (sin usar los datos de prueba).\n",
    "Si tenemos pocos datos, podríamos optar por 10% para validación, pero en general un 15% proporciona un buen equilibrio para evaluar el rendimiento sin sobreajustar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.4. Resumen de las proporciones\n",
    "70% para entrenamiento: Permite que el modelo aprenda de una buena cantidad de datos.\n",
    "15% para validación: Permite ajustar hiperparámetros durante el entrenamiento.\n",
    "15% para prueba: Permite evaluar el rendimiento del modelo de forma objetiva y asegurarse de que no haya sobreajuste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar el tamaño de cada conjunto de datos\n",
    "print(f\"Tamaño de datos de entrenamiento: {X_train.shape[0]} filas\")\n",
    "print(f\"Tamaño de datos de validación: {X_val.shape[0]} filas\")\n",
    "print(f\"Tamaño de datos de prueba: {X_test.shape[0]} filas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.5. Alternativa con validación cruzada (K-fold cross-validation)\n",
    "Si prefieres no tener un conjunto de validación fijo, puedes usar validación cruzada (K-fold cross-validation) para evaluar el modelo durante el entrenamiento. Esta es una buena opción si tienes un conjunto de datos pequeño."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Realizar validación cruzada con 5 pliegues\n",
    "scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "print(f\"Scores de validación cruzada: {scores}\")\n",
    "print(f\"Precisión promedio: {scores.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Por qué usar validación cruzada?\n",
    "La validación cruzada utiliza diferentes subconjuntos del conjunto de entrenamiento para entrenar y validar el modelo varias veces. Esto da una estimación más robusta de cómo el modelo podría comportarse en datos nuevos, especialmente si tenemos pocos datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.6. Conclusión\n",
    "La división de datos en entrenamiento, validación y prueba es crucial para evitar el sobreajuste y garantizar que el modelo generalice correctamente a nuevos datos. Los pasos comunes son:\n",
    "\n",
    "Dividir en entrenamiento (70%) y prueba (30%).\n",
    "Dividir el conjunto de entrenamiento (70%) en entrenamiento (85%) y validación (15%).\n",
    "Usar validación cruzada si tienes pocos datos.\n",
    "Este proceso ayuda a tener datos suficientes para entrenar, ajustar y evaluar el modelo de forma adecuada, manteniendo una buena capacidad de generalización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Selección del Modelo\n",
    "Objetivos:\n",
    "    Elegir un modelo adecuado para el problema y los datos.\n",
    "Acciones:\n",
    "    Determinar el tipo de modelo:\n",
    "        Regresión (si la salida es numérica).\n",
    "        Clasificación (si la salida es categórica).\n",
    "        Clustering o reducción de dimensionalidad (si es no supervisado).\n",
    "Explorar múltiples modelos: Comparar algoritmos como:\n",
    "    Clasificación: Random Forest, SVM, XGBoost.\n",
    "    Regresión: Regresión Lineal, Gradient Boosting.\n",
    "    Clustering: k-Means, DBSCAN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.1. Regresión Lineal (Supervisado - Regresión)\n",
    "Este modelo se usa cuando la variable dependiente es continua."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Datos de ejemplo\n",
    "X = df.drop('target', axis=1)  # Variables independientes\n",
    "y = df['target']  # Variable dependiente\n",
    "\n",
    "# División en entrenamiento y prueba (70% entrenamiento, 30% prueba)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Modelo de regresión lineal\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluación del modelo\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Error cuadrático medio (MSE): {mse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicación:\n",
    "\n",
    "El modelo LinearRegression() ajusta una línea recta a los datos de entrenamiento.\n",
    "Usamos el MSE para evaluar la precisión del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.2. Regresión con Random Forest (Supervisado - Regresión)\n",
    "Este modelo es útil cuando las relaciones no son lineales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Modelo de Random Forest para regresión\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluación del modelo\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "print(f\"Error cuadrático medio (MSE) - Random Forest: {mse_rf}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicación:\n",
    "\n",
    "RandomForestRegressor() crea un conjunto de árboles de decisión y promedia sus predicciones para mejorar la precisión y evitar el sobreajuste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relaciones no son lineales en el contexto de Random Forest (y en general, en machine learning), se refiere a que la relación entre las características (variables independientes) y la variable objetivo (dependiente) no sigue una estructura matemática sencilla, como una línea recta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.3. Clasificación con Regresión Logística (Supervisado - Clasificación)\n",
    "Este modelo es útil para problemas de clasificación binaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Datos de ejemplo\n",
    "X = df.drop('target', axis=1)  # Variables independientes\n",
    "y = df['target']  # Variable dependiente (binaria)\n",
    "\n",
    "# División en entrenamiento y prueba (70% entrenamiento, 30% prueba)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Modelo de regresión logística\n",
    "logreg_model = LogisticRegression()\n",
    "logreg_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred_logreg = logreg_model.predict(X_test)\n",
    "\n",
    "# Evaluación del modelo\n",
    "accuracy = accuracy_score(y_test, y_pred_logreg)\n",
    "print(f\"Precisión de la Regresión Logística: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicación:\n",
    "\n",
    "LogisticRegression() es un modelo lineal usado para predecir probabilidades, ideal para clasificación binaria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.5. XGBoost (Supervisado - Regresión y Clasificación)\n",
    "XGBoost es un potente algoritmo basado en el boosting, usado para problemas tanto de regresión como de clasificación.\n",
    "\n",
    "Código (para Regresión):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Modelo de XGBoost para regresión\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluación del modelo\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "print(f\"Error cuadrático medio (MSE) - XGBoost: {mse_xgb}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Código (para Clasificación):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Modelo de XGBoost para clasificación\n",
    "xgb_clf_model = xgb.XGBClassifier(objective='binary:logistic', n_estimators=100)\n",
    "xgb_clf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred_xgb_clf = xgb_clf_model.predict(X_test)\n",
    "\n",
    "# Evaluación del modelo\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb_clf)\n",
    "print(f\"Precisión de XGBoost (Clasificación): {accuracy_xgb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicación:\n",
    "\n",
    "XGBRegressor() y XGBClassifier() son las implementaciones de XGBoost para problemas de regresión y clasificación respectivamente. XGBoost es muy eficiente para manejar grandes cantidades de datos y evitar el sobreajuste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo del boosting es combinar múltiples modelos (usualmente modelos simples, como árboles de decisión) para formar un modelo final más preciso y robusto.\n",
    "Cada modelo sucesivo corrige los errores cometidos por el modelo anterior. En otras palabras, cada nuevo modelo se enfoca en mejorar las predicciones en los ejemplos que los modelos anteriores han clasificado incorrectamente.\n",
    "\n",
    "Tipos de modeos:\n",
    "- AdaBoost\n",
    "- Gradient Boosting\n",
    "- XGBoost\n",
    "- LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.6. Support Vector Machine (SVM - Supervisado - Clasificación y Regresión)\n",
    "El SVM puede ser utilizado tanto para clasificación como para regresión.\n",
    "\n",
    "Código para Clasificación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Modelo SVM para clasificación\n",
    "svm_model = SVC(kernel='linear', random_state=42)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluación del modelo\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(f\"Precisión del SVM: {accuracy_svm}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Código para Regresión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Modelo SVM para regresión\n",
    "svr_model = SVR(kernel='linear')\n",
    "svr_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred_svr = svr_model.predict(X_test)\n",
    "\n",
    "# Evaluación del modelo\n",
    "mse_svr = mean_squared_error(y_test, y_pred_svr)\n",
    "print(f\"Error cuadrático medio (MSE) - SVM: {mse_svr}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicación:\n",
    "\n",
    "SVC() y SVR() son las versiones de SVM para clasificación y regresión respectivamente. SVM es eficaz en problemas donde las clases no son linealmente separables y cuando el número de características es alto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clases no son linealmente separables, se hace referencia a un escenario en el que las dos clases a predecir no se pueden separar por una línea recta (o un hiperplano en dimensiones superiores) en el espacio de características. Esto significa que no es posible encontrar un único hiperplano que divida de manera perfecta las dos clases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagina un conjunto de datos donde los puntos de una clase están agrupados alrededor de un círculo y los de la otra clase están agrupados en la zona exterior del círculo. Una línea recta no puede separar las clases de manera efectiva en este caso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kernel lineal:\n",
    "Descripción: Este es el kernel más simple y se utiliza cuando las clases son linealmente separables, o cuando quieres que el modelo funcione en el espacio original de características sin aplicar ninguna transformación.\n",
    "Uso: Se usa cuando el conjunto de datos tiene una separación clara y lineal entre las clases.\n",
    "\n",
    "- Kernel polinómico:\n",
    "Descripción: Este kernel mapea los datos a un espacio de mayor dimensión de forma polinómica, donde \n",
    "𝑑\n",
    "d es el grado del polinomio, y \n",
    "𝑐\n",
    "c es un parámetro de sesgo.\n",
    "Uso: Es útil cuando las relaciones entre las clases pueden representarse como una combinación polinómica de las características.\n",
    "\n",
    "- Kernel Radial Basis Function (RBF) o Gaussiano\n",
    "Descripción: El kernel RBF mide la similitud entre dos puntos, tomando en cuenta la distancia euclidiana entre ellos. Este kernel transforma los datos de manera no lineal, lo que permite que el SVM cree una frontera de decisión muy flexible, adecuada para casos de clases no linealmente separables.\n",
    "Uso: Es el kernel más común en situaciones donde las clases no son linealmente separables, ya que puede mapear los datos a un espacio de características de alta dimensión y crear fronteras de decisión complejas y no lineales.\n",
    "\n",
    "- Kernel Sigmoide\n",
    "Descripción: Este kernel se basa en la función sigmoide y se comporta de manera similar a una red neuronal de una sola capa. Sin embargo, a menudo no se utiliza tanto como el RBF.\n",
    "Uso: Es útil en redes neuronales y en situaciones donde se busca modelar la no linealidad en las relaciones de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si tienes motivos para creer que las relaciones entre las características y la clase no pueden separarse por una línea recta (como en el caso de datos de imagen, texto o datos con interacciones complejas), el kernel RBF es una opción común debido a su flexibilidad y su capacidad para manejar datos con patrones complejos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.7. Reducción de Dimensionalidad con PCA (No Supervisado)\n",
    "La reducción de dimensionalidad es útil para manejar datos con muchas variables, manteniendo la mayor cantidad de información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Datos de ejemplo\n",
    "X = df.drop('target', axis=1)\n",
    "\n",
    "# PCA: Reducción a 2 dimensiones\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Visualización de los datos proyectados\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1])\n",
    "plt.title(\"Reducción de Dimensionalidad con PCA\")\n",
    "plt.xlabel(\"Componente Principal 1\")\n",
    "plt.ylabel(\"Componente Principal 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicación:\n",
    "\n",
    "PCA() reduce el número de características, transformando los datos a un espacio de menor dimensión. Es útil cuando se tienen muchas características y se desea simplificar el modelo sin perder demasiada información."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducción de simensionaldiad:\n",
    "- Se reduce el numeor de variables\n",
    "- cuando hay muchas características uqe pued ellevar l sobreajuste\n",
    "- Elimina características irrelevantes\n",
    "- Mejora eficiencia computacional\n",
    "- Previene el sobreajuste\n",
    "- Mejora la visualzaición\n",
    "- Disminuye la dimensionalidad\n",
    "- Tecnicas son\n",
    "    - PCA\n",
    "    - t-SNE\n",
    "    - Autoencoders\n",
    "- Se usa cuando:\n",
    "    - los datos tienen muchas caracteristicas y se desea mejorar la intepretabilidad\n",
    "    cuando las caracteristicas tienen mucha correlacion\n",
    "    cuando se necesita una mejora en la eficiencia computacional\n",
    "    cuando le modelo muestra señales de sobreajuste. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.8. Clustering con K-Means (No Supervisado)\n",
    "El clustering es útil cuando queremos agrupar datos en categorías sin tener etiquetas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Datos de ejemplo\n",
    "X = df.drop('target', axis=1)\n",
    "\n",
    "# K-Means\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Visualización de los clusters\n",
    "plt.scatter(X['feature1'], X['feature2'], c=kmeans.labels_, cmap='viridis')\n",
    "plt.title(\"Clusters con K-Means\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicación:\n",
    "\n",
    "KMeans() agrupa los datos en el número de clusters especificado. Los puntos dentro de cada cluster son más similares entre sí que a los puntos de otros clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.9 KNN (clasificación)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de bibliotecas necesarias\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Cargar datos de ejemplo (Iris dataset)\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Preprocesamiento: Estándarizar las características\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Dividir el conjunto de datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Crear el modelo KNN con k=3\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Entrenar el modelo\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Evaluar el modelo\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Entrenamiento del Modelo\n",
    "Objetivos:\n",
    "    Ajustar los parámetros del modelo a los datos de entrenamiento.\n",
    "Acciones:\n",
    "    Entrenar con el conjunto de entrenamiento.\n",
    "    Ajustar parámetros iniciales.\n",
    "    Guardar modelos iniciales para evaluación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Evaluación Inicial\n",
    "Objetivos:\n",
    "Medir el rendimiento del modelo con datos de validación.\n",
    "Acciones:\n",
    "Métricas comunes:\n",
    "Clasificación: Accuracy, Precision, Recall, F1, ROC-AUC.\n",
    "Regresión: RMSE, MAE, R².\n",
    "Comparar varios modelos: Identificar cuál tiene el mejor desempeño."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Optimización del Modelo\n",
    "Objetivos:\n",
    "Mejorar el rendimiento del modelo.\n",
    "Acciones:\n",
    "Ajuste de hiperparámetros:\n",
    "Usar técnicas como GridSearchCV o RandomizedSearchCV.\n",
    "Reducción de sobreajuste:\n",
    "Regularización (L1, L2).\n",
    "Simplificar el modelo o reducir el número de características.\n",
    "Ensamblado: Combinar modelos para mejorar la precisión (bagging, boosting, stacking)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Evaluación Final con Datos de Prueba\n",
    "Objetivos:\n",
    "Verificar el desempeño en datos no vistos.\n",
    "Acciones:\n",
    "Usar el conjunto de prueba: Obtener métricas finales de rendimiento.\n",
    "Comparar con la línea base (baseline): Evaluar si el modelo supera métodos simples como un promedio o un modelo aleatorio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Implementación\n",
    "Objetivos:\n",
    "Desplegar el modelo en un entorno real para uso práctico.\n",
    "Acciones:\n",
    "Crear una API: Usar Flask, FastAPI o Django para servir el modelo.\n",
    "Automatizar predicciones: Integrar con sistemas existentes.\n",
    "Alojar el modelo: Usar servicios en la nube (AWS, Azure, GCP) o on-premises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Monitoreo y Mantenimiento\n",
    "Objetivos:\n",
    "Asegurar que el modelo sigue siendo útil y preciso.\n",
    "Acciones:\n",
    "Monitoreo de desempeño: Revisar métricas en tiempo real.\n",
    "Identificar data drift: Detectar si los datos de entrada han cambiado.\n",
    "Reentrenar el modelo: Actualizarlo periódicamente con nuevos datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acá termina"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
